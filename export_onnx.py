# -*- coding: utf-8 -*-

import os
import numpy as np
import torch
from net.moce_jdd import MoCEJDD
from net.moce_jdd_isp import MoCEJDD_ISP
from options import train_options
import yaml
import onnxruntime


def export_model(model, dummy_input, output_path):
    """
    å¯¼å‡ºæ¨¡å‹åˆ° ONNX æ ¼å¼ï¼Œå¹¶è®¾ç½®åŠ¨æ€è½´ä»¥æ”¯æŒå¯å˜è¾“å…¥å°ºå¯¸ã€‚
    """
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    model.eval()

    # å®šä¹‰åŠ¨æ€è½´
    # æˆ‘ä»¬å‘Šè¯‰ ONNXï¼Œè¾“å…¥'input'çš„ç¬¬0è½´(batch), ç¬¬3è½´(height), ç¬¬4è½´(width)æ˜¯å¯å˜çš„
    # å¹¶ç»™å®ƒä»¬èµ·äº†å¯è¯»çš„åç§° 'batch_size', 'height', 'width'
    # è¾“å‡º'output'çš„ç›¸åº”ç»´åº¦ä¹Ÿä¼šæ˜¯åŠ¨æ€çš„
    dynamic_axes = {
        'input': {0: 'batch_size', 3: 'height', 4: 'width'},
        'output': {0: 'batch_size', 3: 'height', 4: 'width'}  # å‡è®¾è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦ä¹Ÿéšè¾“å…¥å˜åŒ–
    }

    # å¯¼å‡ºæ¨¡å‹
    print(f"ğŸš€ Starting ONNX export to {output_path}...")
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=17,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes=dynamic_axes,  # <--- æ ¸å¿ƒæ”¹åŠ¨åœ¨è¿™é‡Œ
        training=torch.onnx.TrainingMode.EVAL,
        operator_export_type=torch.onnx.OperatorExportTypes.ONNX,
        verbose=False,
    )
    print(f"âœ… Model exported to {output_path} with dynamic axes.")


def check_determinism(model, dummy_input):
    """
    åœ¨å¯¼å‡ºå‰æµ‹è¯•æ¨¡å‹ç¡®å®šæ€§ã€‚
    """
    model.eval()
    with torch.no_grad():
        out1 = model(dummy_input)
        out2 = model(dummy_input)
        print(f"ğŸ” Checking model determinism. Output difference: {torch.max(torch.abs(out1 - out2)).item()}")


def update_model_params(train_opt, yaml_path='hparams.yaml'):
    """åªæ›´æ–°æ¨¡å‹ç»“æ„ç›¸å…³å‚æ•°"""
    MODEL_KEYS = {
        'stage_depth', 'topk', 'num_blocks', 'num_dec_blocks',
        'num_exp_blocks', 'num_refinement_blocks', 'depth_type',
        'dim', 'heads', 'latent_dim', 'complexity_scale'
    }

    if not os.path.exists(yaml_path):
        print(f"âš ï¸ Warning: hparams.yaml not found at {yaml_path}. Using default parameters.")
        return train_opt

    with open(yaml_path, 'r') as f:
        hparams = yaml.safe_load(f)

    # åªæ›´æ–°æ¨¡å‹ç»“æ„å‚æ•°
    for k in MODEL_KEYS:
        if k in hparams:
            setattr(train_opt, k, hparams[k])

    print("ğŸ”„ Model parameters updated from hparams.yaml.")
    return train_opt

def compare_pytorch_and_onnx(model, onnx_path):
    """
    ç²¾ç¡®å¯¹æ¯” PyTorch æ¨¡å‹å’Œ ONNX æ¨¡å‹çš„è¾“å‡ºã€‚
    """
    print("\n" + "="*50)
    print("ğŸ”¬ Starting Detailed Comparison: PyTorch vs. ONNX")
    print("="*50)

    # 1. å‡†å¤‡å®Œå…¨ç›¸åŒçš„è¾“å…¥æ•°æ®
    # ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„ã€ééšæœºçš„è¾“å…¥ï¼Œæˆ–è€…ä»æ–‡ä»¶ä¸­åŠ è½½ï¼Œä»¥ä¿è¯å¯å¤ç°æ€§
    input_tensor = torch.randn(1, 5, 3, 360, 640, dtype=torch.float32)
    input_numpy = input_tensor.numpy()

    # 2. PyTorch æ¨ç†
    model.eval()
    with torch.no_grad():
        pytorch_output = model(input_tensor)
    pytorch_output_np = pytorch_output.detach().numpy()
    print(f"PyTorch output shape: {pytorch_output_np.shape}")

    # 3. ONNX æ¨ç†
    ort_session = onnxruntime.InferenceSession(onnx_path)
    input_name = ort_session.get_inputs()[0].name
    ort_inputs = {input_name: input_numpy}
    onnx_output_np = ort_session.run(None, ort_inputs)[0]
    print(f"ONNX output shape: {onnx_output_np.shape}")

    # 4. å¯¹æ¯”ç»“æœ
    try:
        np.testing.assert_allclose(pytorch_output_np, onnx_output_np, rtol=1e-3, atol=1e-5)
        print("\nâœ… SUCCESS: Outputs are very close!")
    except AssertionError as e:
        print("\nâŒ FAILURE: Outputs are significantly different.")
        # è®¡ç®—å¹¶æ‰“å°å·®å¼‚
        abs_diff = np.abs(pytorch_output_np - onnx_output_np)
        print(f"  - Max absolute difference: {np.max(abs_diff)}")
        print(f"  - Mean absolute difference: {np.mean(abs_diff)}")
        print(f"  - Max relative difference: {np.max(abs_diff / np.abs(pytorch_output_np))}")
def main_export(opt, model_type):
    """
    ä¸»å‡½æ•°ï¼šåŠ è½½ PyTorch æ¨¡å‹å¹¶å¯¼å‡ºä¸º ONNXã€‚
    """
    np.random.seed(42)
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)  # ä½¿ç”¨ _all ä¿è¯å¤šGPUä¸‹çš„ä¸€è‡´æ€§

    # ç¡®ä¿ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True, warn_only=True)

    if model_type!='isp':

        model = MoCEJDD(
            dim=opt.dim,
            num_blocks=opt.num_blocks,
            num_dec_blocks=opt.num_dec_blocks,
            levels=len(opt.num_blocks),
            heads=opt.heads,
            num_refinement_blocks=opt.num_refinement_blocks,
            topk=opt.topk,
            num_experts=opt.num_exp_blocks,
            rank=opt.latent_dim,
            with_complexity=opt.with_complexity,
            depth_type=opt.depth_type,
            stage_depth=opt.stage_depth,
            rank_type=opt.rank_type,
            complexity_scale=opt.complexity_scale,
        )

    else:
        model = MoCEJDD_ISP(
            dim=opt.dim,
            num_blocks=opt.num_blocks,
            num_dec_blocks=opt.num_dec_blocks,
            levels=len(opt.num_blocks),
            heads=opt.heads,
            num_refinement_blocks=opt.num_refinement_blocks,
            topk=opt.topk,
            num_experts=opt.num_exp_blocks,
            rank=opt.latent_dim,
            with_complexity=opt.with_complexity,
            depth_type=opt.depth_type,
            stage_depth=opt.stage_depth,
            rank_type=opt.rank_type,
            complexity_scale=opt.complexity_scale,
        )
    print("ğŸ› ï¸ Model initialized.")

    # --- åŠ è½½æƒé‡ ---
    if model_type!='isp':
        ckpt_path = os.path.join(opt.ckpt_dir, 'MoCE_JDD', "last.ckpt")
    else:
        ckpt_path = os.path.join(opt.ckpt_dir, 'MoCE_JDD', "last_isp.ckpt")
    if not os.path.exists(ckpt_path):
        print(f"âŒ Error: Checkpoint file not found at {ckpt_path}")
        print(
            "Please ensure 'options.py' has the correct 'ckpt_dir' and 'checkpoint_id', or provide them as arguments.")
        return  # æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶åˆ™é€€å‡º

    checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))  # æ¨èåœ¨CPUä¸ŠåŠ è½½ä»¥é¿å…GPUå†…å­˜é—®é¢˜
    model_state_dict = {k.replace('net.', ''): v for k, v in checkpoint['state_dict'].items()
                        if k.startswith('net.')}
    model.load_state_dict(model_state_dict, strict=True)
    print(f"âœ… Weights loaded from {ckpt_path}")

    model.eval()

    # --- åˆ›å»ºç”¨äºå¯¼å‡ºçš„è™šæ‹Ÿè¾“å…¥ ---
    # å°ºå¯¸ï¼š (batch, time, channels, height, width)
    dummy_input = torch.rand(1, 5, 3, 720, 1280, dtype=torch.float32)  # ä½¿ç”¨ 360p (640x360) ä½œä¸ºå¯¼å‡ºåŸºå‡†

    # --- æ£€æŸ¥ä¸å¯¼å‡º ---
    check_determinism(model, dummy_input)
    export_model(model, dummy_input, f"onnx/model{model_type}.onnx")

    compare_pytorch_and_onnx(model, f"onnx/model{model_type}.onnx")


def test_onnx_model(onnx_path="model.onnx"):
    """
    åŠ è½½ ONNX æ¨¡å‹å¹¶ä½¿ç”¨ 720p çš„éšæœºè¾“å…¥è¿›è¡Œæ¨ç†æµ‹è¯•ã€‚

    Args:
        onnx_path (str): ONNX æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    print("\n" + "=" * 50)
    print("âš¡ Starting ONNX Runtime Test")
    print("=" * 50)

    if not os.path.exists(onnx_path):
        print(f"âŒ Error: ONNX model not found at '{onnx_path}'. Cannot run test.")
        return

    try:
        # 1. åˆ›å»º ONNX Runtime æ¨ç†ä¼šè¯
        ort_session = onnxruntime.InferenceSession(onnx_path)
        print("âœ… ONNX Runtime session created successfully.")

        # 2. è·å–æ¨¡å‹è¾“å…¥çš„åç§°
        input_name = ort_session.get_inputs()[0].name
        print(f"ğŸ” Model input name: '{input_name}'")

        # 3. å‡†å¤‡ä¸€ä¸ªæ–°çš„720pæµ‹è¯•è¾“å…¥
        # å°ºå¯¸ï¼š(batch, time, channels, height, width)
        # 720p: 1280x720
        test_input_shape_720p = (1, 5, 3, 720, 1280)
        test_input_720p = np.random.rand(*test_input_shape_720p).astype(np.float32)
        print(f"ğŸ“¦ Prepared a new random input with shape (720p): {test_input_720p.shape}")

        # 4. æ‰§è¡Œæ¨ç†
        print("ğŸš€ Running inference with the new 720p input...")
        ort_inputs = {input_name: test_input_720p}
        ort_outs = ort_session.run(None, ort_inputs)
        output_tensor = ort_outs[0]

        # 5. æ‰“å°è¾“å‡ºä¿¡æ¯
        print("\nğŸ‰ Inference successful!")
        print(f"âœ… Output tensor shape: {output_tensor.shape}")
        print(f"âœ… Output data type: {output_tensor.dtype}")

    except Exception as e:
        print(f"âŒ An error occurred during ONNX Runtime test: {e}")


if __name__ == '__main__':
    model_type = 'isp' # ''/isp
    # è§£æå‘½ä»¤è¡Œå‚æ•°æˆ–ä½¿ç”¨é»˜è®¤å€¼
    train_opt = train_options()

    # ä» checkpoints/<id>/hparams.yaml æ›´æ–°æ¨¡å‹å‚æ•°
    if model_type != 'isp':
        hparams_path = os.path.join('checkpoints', train_opt.checkpoint_id, 'hparams.yaml')
    else:
        hparams_path = os.path.join('checkpoints', train_opt.checkpoint_id, 'hparams_isp.yaml')


    train_opt = update_model_params(train_opt, hparams_path)

    # æ‰§è¡Œå¯¼å‡º
    main_export(train_opt,model_type)

    # --- æ­¥éª¤ 2: è¯»å–å¯¼å‡ºçš„ ONNX æ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯• ---
    test_onnx_model(onnx_path=f"onnx/model{model_type}.onnx")